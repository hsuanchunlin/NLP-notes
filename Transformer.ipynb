{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Model:\n",
    "<figure>\n",
    "<img src=images/transformer/model.png alt=\"drawing\" width=\"400\"/>\n",
    "<figcaption><b>Reference: <b></figcaption>\n",
    "</figure>\n",
    "\n",
    "## Blocks in encoder layer\n",
    "<figure>\n",
    "<img src=images/transformer/encoder-layer.png>\n",
    "</figure>\n",
    "\n",
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow_datasets\n",
    "#pip install -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled product attention\n",
    "<figure>\n",
    "<img src=./images/transformer/self-attention-part.png width= 300/>\n",
    "<figcaption>Precious Metal Price Prediction Based on Deep Regularization Self-Attention Regression</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) # Shape = (..., seq_len_q, seq_len_k)\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # calculate matmul_qk_v\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to combind two masks into one mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "#### Scaled product attention\n",
    "<figure>\n",
    "<img src=./images/transformer/mha_img_original.png width= 500/>\n",
    "<figcaption>Precious Metal Price Prediction Based on Deep Regularization Self-Attention Regression</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multi-head attention layer\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        #Because for multi-head, head number * depth = multi-head\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #Set layers for q, k, v\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) # shape = (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test multi-head attention (MHA) layer\n",
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point wise feed forward network\n",
    "\n",
    "Point wise feed forward network consists of **two fully-connected layers** with a ReLU activation in between.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Encoder layer\n",
    "<figure>\n",
    "<img src=images/transformer/encoder-layer.png>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        #define layers\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "<figure>\n",
    "<img src=images/transformer/decoder-layer.png width=200 />\n",
    "</figure>\n",
    "\n",
    "- Two multi-head attention\n",
    "- Three add and norm\n",
    "- One feed forward\n",
    "\n",
    "Description from tensorflow:\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "- Masked multi-head attention (with look ahead mask and padding mask)\n",
    "- Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer.\n",
    "- Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is **LayerNorm(x + Sublayer(x))**. The normalization is done on the d_model (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "- Q receives the output from decoder's first attention block\n",
    "- K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output.\n",
    "\n",
    "In other words, the decoder predicts the next token by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "\n",
    "        # Residual\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # Here in the second attention layer, Q and K are enc_output and V is out1\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "\n",
    "        # Residual\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        \n",
    "        # Residual\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trial\n",
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's time to assemble our Encoder\n",
    "<figure>\n",
    "<img src=images/transformer/model.png alt=\"drawing\" width=\"400\"/>\n",
    "<figcaption><b>Reference: <b></figcaption>\n",
    "</figure>\n",
    "\n",
    "### Position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rate = 1/np.power(10000, (2*(i/2))/np.float32(d_model))\n",
    "    return pos*angle_rate\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048, 512)\n"
     ]
    }
   ],
   "source": [
    "n, d = 2048, 512\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #how many encoder layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        #encoding and position encoding\n",
    "        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        \n",
    "        return x # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "<figure>\n",
    "<img src=images/transformer/model.png alt=\"drawing\" width=\"400\"/>\n",
    "<figcaption><b>Reference: <b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model,  num_heads, dff, target_vocab_size,\n",
    "    maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        #Here the layer settings are similar as Encoder\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x) # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training )\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input,\n",
    "                              enc_output=sample_encoder_output,\n",
    "                              training=False,\n",
    "                              look_ahead_mask=None,\n",
    "                              padding_mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can have the total Transformer model by combind Encoder and Decoder\n",
    "\n",
    "There are encoder, decoder, which we just constructed, and one final Dense layer for exporting result.\n",
    "\n",
    "In addition, there is also a function included in class Transformer to create masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "               super().__init__()\n",
    "               #Here pe_imput is maximum_position_encoding\n",
    "               self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                                      input_vocab_size, pe_input, rate)\n",
    "               \n",
    "               self.decoder = Decoder(num_layers, d_model, num_heads,\n",
    "                                      dff,target_vocab_size, pe_target, rate)\n",
    "\n",
    "               self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask) \n",
    "        # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training,\n",
    "                                                     look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        #Encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    input_vocab_size=8500, target_vocab_size=8000,\n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer with a custom learning rate scheduler according to the following:\n",
    "$$\n",
    "lrate = d^{-0.5}_{model}*min(step_num^{-0.5}, step\\_ num \\cdot warmup\\_ steps^{-1.5})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self,step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "**SparseCategoricalCrossentropy**\n",
    "\n",
    "Since the target sequences are padded, it is important to **apply a padding mask** when calculating the loss.\n",
    "Here is how to customize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    #mask = real is not 0\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    #find mask and accuracies are both True/False.\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and checkpointing\n",
    "\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 15:03:52.433856: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 124.94 MiB (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /Users/fatmimi/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:16<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:17<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:18<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:19<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:20<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:21<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:22<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:23<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:25<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:26<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:27<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:28<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:29<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:30<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:31<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:32<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:33<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:34<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:35<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:36<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:38<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:39<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:40<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:41<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:42<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:43<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:44<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:45<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:46<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:47<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:48<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:50<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:50<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:52<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:53<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:54<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:55<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:56<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:57<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:58<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:59<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [01:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [01:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [01:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [01:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [01:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [01:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [01:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [01:08<?, ? url/s]\n",
      "Extraction completed...: 0 file [01:08, ? file/s]\n",
      "Dl Size...:  51%|     | 63/124 [01:08<01:06,  1.09s/ MiB]\n",
      "Dl Completed...:   0%|          | 0/1 [01:08<?, ? url/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m examples, metadata \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mted_hrlr_translate/pt_to_en\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_examples, val_examples \u001b[38;5;241m=\u001b[39m examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow_datasets/core/load.py:318\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m    317\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 318\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py:439\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m   \u001b[38;5;66;03m# Old version of TF are not os.PathLike compatible\u001b[39;00m\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf_compat\u001b[38;5;241m.\u001b[39mmock_gfile_pathlike():\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    445\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    446\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    447\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py:1113\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1112\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1113\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_pipeline_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[1;32m   1119\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[1;32m   1120\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[1;32m   1121\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[1;32m   1122\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow_datasets/translate/ted_hrlr.py:121\u001b[0m, in \u001b[0;36mTedHrlrTranslate._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[0;32m--> 121\u001b[0m   dl_dir \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_DATA_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m   source, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_config\u001b[38;5;241m.\u001b[39mlanguage_pair\n\u001b[1;32m    124\u001b[0m   data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dl_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_to_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (source, target))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow_datasets/core/download/download_manager.py:634\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[1;32m    633\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow_datasets/core/download/download_manager.py:767\u001b[0m, in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/tensorflow_datasets/core/download/download_manager.py:767\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m res \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises)  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/promise/promise.py:511\u001b[0m, in \u001b[0;36mPromise.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# type: (Optional[float]) -> T\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEFAULT_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_settled_value(_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/promise/promise.py:506\u001b[0m, in \u001b[0;36mPromise._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# type: (Optional[float]) -> None\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/promise/promise.py:502\u001b[0m, in \u001b[0;36mPromise.wait\u001b[0;34m(cls, promise, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mcls\u001b[39m, promise, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# type: (Promise, Optional[float]) -> None\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[43masync_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpromise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/promise/async_.py:117\u001b[0m, in \u001b[0;36mAsync.wait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m promise\u001b[38;5;241m.\u001b[39mis_pending:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;66;03m# We return if the promise is already\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# fulfilled or rejected\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/envs/mactf-sandbox/lib/python3.8/site-packages/promise/schedulers/immediate.py:25\u001b[0m, in \u001b[0;36mImmediateScheduler.wait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m     22\u001b[0m     e\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m     24\u001b[0m promise\u001b[38;5;241m.\u001b[39m_then(on_resolve_or_reject, on_resolve_or_reject)\n\u001b[0;32m---> 25\u001b[0m waited \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m waited:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pt_examples, en_examples \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_examples\u001b[49m\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m pt \u001b[38;5;129;01min\u001b[39;00m pt_examples\u001b[38;5;241m.\u001b[39mnumpy():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pt\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_examples' is not defined"
     ]
    }
   ],
   "source": [
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "  for pt in pt_examples.numpy():\n",
    "    print(pt.decode('utf-8'))\n",
    "\n",
    "  print()\n",
    "\n",
    "  for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenizations (use google's tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/models/ted_hrlr_translate_pt_en_converter.zip\n",
      "188416/184801 [==============================] - 0s 1us/step\n",
      "196608/184801 [===============================] - 0s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./ted_hrlr_translate_pt_en_converter.zip'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"ted_hrlr_translate_pt_en_converter\"\n",
    "tf.keras.utils.get_file(\n",
    "    f\"{model_name}.zip\",\n",
    "    f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\",\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 15:06:53.891237: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-03 15:06:53.900949: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "tokenizers = tf.saved_model.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'get_reserved_tokens',\n",
       " 'get_vocab_path',\n",
       " 'get_vocab_size',\n",
       " 'lookup',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_batches\u001b[39m(ds):\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m       ds\n\u001b[1;32m     14\u001b[0m       \u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[38;5;241m.\u001b[39mmap(tokenize_pairs, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     18\u001b[0m       \u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE))\n\u001b[0;32m---> 21\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m make_batches(\u001b[43mtrain_examples\u001b[49m)\n\u001b[1;32m     22\u001b[0m val_batches \u001b[38;5;241m=\u001b[39m make_batches(val_examples)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_examples' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize_pairs(pt, en):\n",
    "    pt = tokenizers.pt.tokenize(pt)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    pt = pt.to_tensor()\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    en = en.to_tensor()\n",
    "    return pt, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_batches\u001b[39m(ds):\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m       ds\n\u001b[1;32m      6\u001b[0m       \u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[38;5;241m.\u001b[39mmap(tokenize_pairs, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     10\u001b[0m       \u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE))\n\u001b[0;32m---> 12\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m make_batches(\u001b[43mtrain_examples\u001b[49m)\n\u001b[1;32m     13\u001b[0m val_batches \u001b[38;5;241m=\u001b[39m make_batches(val_examples)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_examples' is not defined"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Create Transformer model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m transformer \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[1;32m     10\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[1;32m     11\u001b[0m     d_model\u001b[38;5;241m=\u001b[39md_model,\n\u001b[1;32m     12\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mnum_heads,\n\u001b[1;32m     13\u001b[0m     dff\u001b[38;5;241m=\u001b[39mdff,\n\u001b[0;32m---> 14\u001b[0m     input_vocab_size\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizers\u001b[49m\u001b[38;5;241m.\u001b[39mpt\u001b[38;5;241m.\u001b[39mget_vocab_size()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     15\u001b[0m     target_vocab_size\u001b[38;5;241m=\u001b[39mtokenizers\u001b[38;5;241m.\u001b[39men\u001b[38;5;241m.\u001b[39mget_vocab_size()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     16\u001b[0m     pe_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     17\u001b[0m     pe_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     18\u001b[0m     rate\u001b[38;5;241m=\u001b[39mdropout_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizers' is not defined"
     ]
    }
   ],
   "source": [
    "# Set hyper parameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "#Create Transformer model\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer([inp, tar_inp],\n",
    "                                 training = True)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "    train_step(inp, tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Training, we can create the translator and apply the model to see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, tokenizers, transformer):\n",
    "        self.tokenizers = tokenizers\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, sentence, max_length=20):\n",
    "        # input sentence is portuguese, hence adding the start and end token\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "            sentence = sentence[tf.newaxis]\n",
    "\n",
    "        sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "        encoder_input = sentence\n",
    "\n",
    "        # as the target is english, the first token to the transformer should be the\n",
    "        # english start token.\n",
    "        start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        # `tf.TensorArray` is required here (instead of a python list) so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions, _ = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "        # select the last token from the seq_len dimension\n",
    "            predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # output.shape (1, tokens)\n",
    "        text = tokenizers.en.detokenize(output)[0]  # shape: ()\n",
    "\n",
    "        tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop. So recalculate them outside\n",
    "        # the loop.\n",
    "        _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "\n",
    "        return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m translator \u001b[38;5;241m=\u001b[39m Translator(tokenizers, \u001b[43mtransformer\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_translation\u001b[39m(sentence, tokens, ground_truth):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m15s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "translator = Translator(tokenizers, transformer)\n",
    "def print_translation(sentence, tokens, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')\n",
    "\n",
    "sentence = \"este  um problema que temos que resolver.\"\n",
    "ground_truth = \"this is a problem we have to solve .\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"este  o primeiro livro que eu fiz.\"\n",
    "ground_truth = \"this is the first book i've ever done.\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
    "  # The plot is of the attention when a token was generated.\n",
    "  # The model didn't generate `<START>` in the output. Skip it.\n",
    "  translated_tokens = translated_tokens[1:]\n",
    "\n",
    "  ax = plt.gca()\n",
    "  ax.matshow(attention)\n",
    "  ax.set_xticks(range(len(in_tokens)))\n",
    "  ax.set_yticks(range(len(translated_tokens)))\n",
    "\n",
    "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
    "  ax.set_xticklabels(\n",
    "      labels, rotation=90)\n",
    "\n",
    "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
    "  ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head = 0\n",
    "# shape: (batch=1, num_heads, seq_len_q, seq_len_k)\n",
    "attention_heads = tf.squeeze(\n",
    "  attention_weights['decoder_layer4_block2'], 0)\n",
    "attention = attention_heads[head]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in_tokens = tf.convert_to_tensor([sentence])\n",
    "in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
    "in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
    "in_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_head(in_tokens, translated_tokens, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For multiple head attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
    "  in_tokens = tf.convert_to_tensor([sentence])\n",
    "  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
    "  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
    "  in_tokens\n",
    "\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "  for h, head in enumerate(attention_heads):\n",
    "    ax = fig.add_subplot(2, 4, h+1)\n",
    "\n",
    "    plot_attention_head(in_tokens, translated_tokens, head)\n",
    "\n",
    "    ax.set_xlabel(f'Head {h+1}')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_weights(sentence, translated_tokens, attention_weights['decoder_layer4_block2'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Eu li sobre triceratops na enciclopdia.\"\n",
    "ground_truth = \"I read about triceratops in the encyclopedia.\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)\n",
    "\n",
    "plot_attention_weights(sentence, translated_tokens,\n",
    "                       attention_weights['decoder_layer4_block2'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model for application\n",
    "That inference model is working, so next you'll export it as a tf.saved_model.\n",
    "\n",
    "To do that, wrap it in yet another tf.Module sub-class, this time with a tf.function on the __call__ method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def __call__(self, sentence):\n",
    "    (result, \n",
    "     tokens,\n",
    "     attention_weights) = self.translator(sentence, max_length=100)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)\n",
    "translator(\"este  o primeiro livro que eu fiz.\").numpy()\n",
    "tf.saved_model.save(translator, export_dir='translator')\n",
    "reloaded = tf.saved_model.load('translator')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef8b9add1314e237b60b92b1a2f6c7f9f0ba0c102b9e1da395e3695f51599e7c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
