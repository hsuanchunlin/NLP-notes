{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Model:\n",
    "<figure>\n",
    "<img src=images/transformer/model.png alt=\"drawing\" width=\"400\"/>\n",
    "<figcaption><b>Reference: <b></figcaption>\n",
    "</figure>\n",
    "\n",
    "## Blocks in encoder layer\n",
    "<figure>\n",
    "<img src=images/transformer/encoder-layer.png>\n",
    "</figure>\n",
    "\n",
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow_datasets\n",
    "#pip install -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled product attention\n",
    "<figure>\n",
    "<img src=./images/transformer/self-attention-part.png width= 300/>\n",
    "<figcaption>Precious Metal Price Prediction Based on Deep Regularization Self-Attention Regression</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) # Shape = (..., seq_len_q, seq_len_k)\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # calculate matmul_qk_v\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to combind two masks into one mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "#### Scaled product attention\n",
    "<figure>\n",
    "<img src=./images/transformer/mha_img_original.png width= 500/>\n",
    "<figcaption>Precious Metal Price Prediction Based on Deep Regularization Self-Attention Regression</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multi-head attention layer\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        #Because for multi-head, head number * depth = multi-head\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #Set layers for q, k, v\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) # shape = (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: AMD Radeon Pro 560X\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 2.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-02 14:06:35.312414: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-02 14:06:35.313542: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-02 14:06:35.314333: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test multi-head attention (MHA) layer\n",
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point wise feed forward network\n",
    "\n",
    "Point wise feed forward network consists of **two fully-connected layers** with a ReLU activation in between.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Encoder layer\n",
    "<figure>\n",
    "<img src=images/transformer/encoder-layer.png>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        #define layers\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "<figure>\n",
    "<img src=images/transformer/decoder-layer.png width=200 />\n",
    "</figure>\n",
    "\n",
    "- Two multi-head attention\n",
    "- Three add and norm\n",
    "- One feed forward\n",
    "\n",
    "Description from tensorflow:\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "- Masked multi-head attention (with look ahead mask and padding mask)\n",
    "- Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer.\n",
    "- Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is **LayerNorm(x + Sublayer(x))**. The normalization is done on the d_model (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "- Q receives the output from decoder's first attention block\n",
    "- K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output.\n",
    "\n",
    "In other words, the decoder predicts the next token by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "\n",
    "        # Residual\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # Here in the second attention layer, Q and K are enc_output and V is out1\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "\n",
    "        # Residual\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        \n",
    "        # Residual\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trial\n",
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's time to assemble our Encoder\n",
    "<figure>\n",
    "<img src=images/transformer/model.png alt=\"drawing\" width=\"400\"/>\n",
    "<figcaption><b>Reference: <b></figcaption>\n",
    "</figure>\n",
    "\n",
    "### Position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rate = 1/np.power(10000, (2*(i/2))/np.float32(d_model))\n",
    "    return pos*angle_rate\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048, 512)\n"
     ]
    }
   ],
   "source": [
    "n, d = 2048, 512\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #how many encoder layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        #encoding and position encoding\n",
    "        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        \n",
    "        return x # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "<figure>\n",
    "<img src=images/transformer/model.png alt=\"drawing\" width=\"400\"/>\n",
    "<figcaption><b>Reference: <b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model,  num_heads, dff, target_vocab_size,\n",
    "    maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        #Here the layer settings are similar as Encoder\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x) # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training )\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input,\n",
    "                              enc_output=sample_encoder_output,\n",
    "                              training=False,\n",
    "                              look_ahead_mask=None,\n",
    "                              padding_mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can have the total Transformer model by combind Encoder and Decoder\n",
    "\n",
    "There are encoder, decoder, which we just constructed, and one final Dense layer for exporting result.\n",
    "\n",
    "In addition, there is also a function included in class Transformer to create masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "               super().__init__()\n",
    "               #Here pe_imput is maximum_position_encoding\n",
    "               self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                                      input_vocab_size, pe_input, rate)\n",
    "               \n",
    "               self.decoder = Decoder(num_layers, d_model, num_heads,\n",
    "                                      dff,target_vocab_size, pe_target, rate)\n",
    "\n",
    "               self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask) \n",
    "        # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training,\n",
    "                                                     look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        #Encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    input_vocab_size=8500, target_vocab_size=8000,\n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer with a custom learning rate scheduler according to the following:\n",
    "$$\n",
    "lrate = d^{-0.5}_{model}*min(step_num^{-0.5}, step\\_ num \\cdot warmup\\_ steps^{-1.5})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self,step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "**SparseCategoricalCrossentropy**\n",
    "\n",
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
    "Here is how to customize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef8b9add1314e237b60b92b1a2f6c7f9f0ba0c102b9e1da395e3695f51599e7c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
