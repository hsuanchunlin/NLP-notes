{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c0289d-5400-4f1c-9af8-3d780dacc9c1",
   "metadata": {},
   "source": [
    "# Transformer - Masking and Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea57371-8f00-4188-8b1b-bbab651907b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7693a31-9145-4c6e-86ae-4109691414e7",
   "metadata": {},
   "source": [
    "## Masking\n",
    "There are two types of masks.\n",
    "1. padding_mask \\\n",
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n",
    "\n",
    "1. look-ahead mask \\\n",
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a1929a-ba78-4d34-b71b-7b9e1fe75ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    #add extra dimensions to add the padding - brocasting\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa10c44-7c06-406a-bc55-93ebb89aa5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de253c04-aa1e-4644-9336-6a4900d1e980",
   "metadata": {},
   "source": [
    "### look-ahead mask\n",
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac155a6d-064d-466d-a39c-28dc34c268fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d584e89-2724-45c6-84fc-3f6ad9128460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1]) #Here use 3\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00025468-0ea3-446a-8873-d45021f882c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multihead Attention - Step by Step \n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "- Linear layers.\n",
    "- Scaled dot-product attention.\n",
    "- Final linear layer.\n",
    "\n",
    "Each multi-head attention block gets three inputs:\n",
    "1. Q (query) - Just like what we search in goole, each search query.\n",
    "1. K (key) - The following results.\n",
    "1. V (value) - \n",
    "\n",
    "$$\n",
    "\\alpha = Q \\cdot K\n",
    "$$\n",
    "Attention score $ \\alpha $ : relationship.\n",
    "\n",
    "These are put through linear (Dense) layers before the multi-head attention function. \\\n",
    "For simplicity/efficiency the code below implements this using a single dense layer with num_heads times as many outputs.\n",
    "- The output is rearranged to a shape of (batch, **num_heads**, ...) before applying the attention function.\n",
    "\n",
    "The scaled_dot_product_attention function defined above is applied in a single call, broadcasted for efficiency. An appropriate mask must be used in the attention step. The attention output for each head is then concatenated (using tf.transpose, and tf.reshape) and put through a final Dense layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information from different representation subspaces at different positions. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad978122-b9f9-47ca-aa7e-423321ddebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wq. wk, wv\n",
    "wq = tf.keras.layers.Dense(d_model)\n",
    "wk = tf.keras.layers.Dense(d_model)\n",
    "wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "#Calculate q, k, v\n",
    "q = wq(q)  # (batch_size, seq_len, d_model)\n",
    "k = wk(k)  # (batch_size, seq_len, d_model)\n",
    "v = wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
