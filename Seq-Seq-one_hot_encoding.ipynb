{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq-Seq model - one hot encoding - from keras tutorial \n",
    "\n",
    "## Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)',\n",
       " 'Go.\\tBouge !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)',\n",
       " 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)',\n",
       " 'Hi.\\tSalut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path= \"./fra.txt\"\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "lines[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000  # Number of samples to train on.\n",
    "test = []\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    \n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    test.append(input_text)\n",
    "\n",
    "    #Make vectorization table by alphabets\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "            \n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 92\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "#Calculate the max length of each sentence for padding\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create token dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoder and padding\n",
    "\n",
    "3D nparray\n",
    "[number of sentence, ,fixed sentence length, number of tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape = (10000, 15, 71)\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0 #Padding Encoder data\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens), name='encoder_inputs' ) #(None, Number of unique input tokens) = (None, None, 71)\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True, name= 'encoder') #encoding Spaces\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs) #encoder_outputs.shape = (None, latent_dim), \n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) #decoder_outputs.shape = [None, max_length of output, 256]\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\", name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs) #Shape = [None, max_length of output, Number of unique output tokens] (None,59,92)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#Decoder output is an one-hot encoding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 20s 148ms/step - loss: 1.1380 - accuracy: 0.7362 - val_loss: 1.0238 - val_accuracy: 0.7145\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.8081 - accuracy: 0.7797 - val_loss: 0.8190 - val_accuracy: 0.7729\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 16s 126ms/step - loss: 0.6518 - accuracy: 0.8181 - val_loss: 0.6874 - val_accuracy: 0.8038\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.5671 - accuracy: 0.8352 - val_loss: 0.6328 - val_accuracy: 0.8184\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.5195 - accuracy: 0.8480 - val_loss: 0.5979 - val_accuracy: 0.8279\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.4844 - accuracy: 0.8576 - val_loss: 0.5519 - val_accuracy: 0.8367\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 16s 130ms/step - loss: 0.4561 - accuracy: 0.8649 - val_loss: 0.5360 - val_accuracy: 0.8423\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.4334 - accuracy: 0.8711 - val_loss: 0.5143 - val_accuracy: 0.8482\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.4134 - accuracy: 0.8765 - val_loss: 0.5069 - val_accuracy: 0.8502\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 16s 132ms/step - loss: 0.3955 - accuracy: 0.8811 - val_loss: 0.4925 - val_accuracy: 0.8550\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.3796 - accuracy: 0.8861 - val_loss: 0.4822 - val_accuracy: 0.8573\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.3645 - accuracy: 0.8900 - val_loss: 0.4751 - val_accuracy: 0.8597\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.3506 - accuracy: 0.8943 - val_loss: 0.4676 - val_accuracy: 0.8623\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.3371 - accuracy: 0.8986 - val_loss: 0.4634 - val_accuracy: 0.8643\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.3251 - accuracy: 0.9018 - val_loss: 0.4531 - val_accuracy: 0.8672\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.3132 - accuracy: 0.9056 - val_loss: 0.4546 - val_accuracy: 0.8676\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 17s 133ms/step - loss: 0.3024 - accuracy: 0.9082 - val_loss: 0.4520 - val_accuracy: 0.8690\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 16s 132ms/step - loss: 0.2912 - accuracy: 0.9122 - val_loss: 0.4502 - val_accuracy: 0.8696\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 16s 132ms/step - loss: 0.2815 - accuracy: 0.9150 - val_loss: 0.4512 - val_accuracy: 0.8700\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.2716 - accuracy: 0.9178 - val_loss: 0.4495 - val_accuracy: 0.8709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f9752a2a160> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f9760fde6d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used trained model to test the translation\n",
    "### Run inference (sampling)\n",
    "1. encode input and retrieve initial decoder state\n",
    "2. run one step of decoder with this initial state\n",
    "and a \"start of sequence\" token as target.\n",
    "Output will be the next target token.\n",
    "3. Repeat with the current target token and current states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None, 71)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None, 92)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder (LSTM)                 [(None, 256),        335872      ['encoder_inputs[0][0]']         \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 256),  357376      ['decoder_inputs[0][0]',         \n",
      "                                 (None, 256),                     'encoder[0][1]',                \n",
      "                                 (None, 256)]                     'encoder[0][2]']                \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)          (None, None, 92)     23644       ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 716,892\n",
      "Trainable params: 716,892\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"s2s\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Encoder for states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the model and construct the encoder and decoder.\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare for state input from previoius Encoder model\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "#Reconstruct decoder model\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Reverse Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    #For the first alphabet\n",
    "    # Generate empty target sequence of length 1 with all 0\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'ai contir.\n",
      "\n",
      "-\n",
      "Input sentence: I'm no saint.\n",
      "Decoded sentence: Je ne suis pas seule.\n",
      "\n",
      "-\n",
      "Input sentence: Keep reading.\n",
      "Decoded sentence: Continue à derlare.\n",
      "\n",
      "-\n",
      "Input sentence: Quit gambling.\n",
      "Decoded sentence: Dis chante le soinnerais.\n",
      "\n",
      "-\n",
      "Input sentence: Taste it.\n",
      "Decoded sentence: Goûtez-le.\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: Garde-le.\n",
      "\n",
      "-\n",
      "Input sentence: Are you happy?\n",
      "Decoded sentence: Êtes-vous sûres ?\n",
      "\n",
      "-\n",
      "Input sentence: He's so cute.\n",
      "Decoded sentence: Il est trop simé.\n",
      "\n",
      "-\n",
      "Input sentence: You can do it.\n",
      "Decoded sentence: Vous avez l'air grasses.\n",
      "\n",
      "-\n",
      "Input sentence: They swam.\n",
      "Decoded sentence: Ils sont la biens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item = np.random.choice(10000, 10)\n",
    "for seq_index in item:\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef8b9add1314e237b60b92b1a2f6c7f9f0ba0c102b9e1da395e3695f51599e7c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
